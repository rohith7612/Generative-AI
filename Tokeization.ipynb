{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSka0SPuSaT_",
        "outputId": "b67d5b3e-941a-43c1-cf0c-5c789664aacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ifubYE5PSfJu"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"\n",
        "Hello! My name is Rohith. I’m learning NLP, Machine Learning, and Compiler Design.\n",
        "Tokenisation helps computers understand text.\n",
        "Let’s test: How many tokens are in this sentence?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gqnKIIGTFuL"
      },
      "source": [
        "# Sentence Tokenization is being done. Corpus is being converted to sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO6O7W69Slv7",
        "outputId": "d9c471d2-43ad-44c3-ca38-4d334e4fc16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hello!\n",
            "My name is Rohith.\n",
            "I’m learning NLP, Machine Learning, and Compiler Design.\n",
            "Tokenisation helps computers understand text.\n",
            "Let’s test: How many tokens are in this sentence?\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)\n",
        "for sentence in documents:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol8jqh1zUCDC"
      },
      "source": [
        "# Word Tokenization is being done. Documents is being converted to words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4CjikqSSu_M",
        "outputId": "75e46a9b-5b44-4204-89c9-5e0362f3ebf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Rohith', '.']\n",
            "['I', '’', 'm', 'learning', 'NLP', ',', 'Machine', 'Learning', ',', 'and', 'Compiler', 'Design', '.']\n",
            "['Tokenisation', 'helps', 'computers', 'understand', 'text', '.']\n",
            "['Let', '’', 's', 'test', ':', 'How', 'many', 'tokens', 'are', 'in', 'this', 'sentence', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pdp5JeIVRWt",
        "outputId": "2f807f07-3b65-465b-ddf0-0b344bf6f189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Rohith', '.']\n",
            "['I', '’', 'm', 'learning', 'NLP', ',', 'Machine', 'Learning', ',', 'and', 'Compiler', 'Design', '.']\n",
            "['Tokenisation', 'helps', 'computers', 'understand', 'text', '.']\n",
            "['Let', '’', 's', 'test', ':', 'How', 'many', 'tokens', 'are', 'in', 'this', 'sentence', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Vb0ZZQVWWT",
        "outputId": "13fb46dd-3152-4e3c-de2f-69e25366a7c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Rohith.',\n",
              " 'I’m',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " ',',\n",
              " 'and',\n",
              " 'Compiler',\n",
              " 'Design.',\n",
              " 'Tokenisation',\n",
              " 'helps',\n",
              " 'computers',\n",
              " 'understand',\n",
              " 'text.',\n",
              " 'Let’s',\n",
              " 'test',\n",
              " ':',\n",
              " 'How',\n",
              " 'many',\n",
              " 'tokens',\n",
              " 'are',\n",
              " 'in',\n",
              " 'this',\n",
              " 'sentence',\n",
              " '?']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLKFxO4EYLAo"
      },
      "source": [
        "# Stemming is being done. PorterStemmer is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qpSiDlyzVb6Y"
      },
      "outputs": [],
      "source": [
        "words = [\n",
        "    \"connect\", \"connected\", \"connecting\", \"connection\",\n",
        "    \"compute\", \"computer\", \"computing\", \"computation\",\n",
        "    \"run\", \"running\", \"runner\", \"runs\",\n",
        "    \"analysis\", \"analyzing\", \"analyzed\", \"analytical\",\n",
        "    \"teach\", \"teacher\", \"teaching\", \"taught\",\n",
        "    \"play\", \"playing\", \"played\", \"player\",\n",
        "    \"happy\", \"happiness\", \"happily\",\n",
        "    \"study\", \"studies\", \"studying\", \"studied\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXYhwMDYXysQ",
        "outputId": "d79a87c8-9b8e-4645-d1bd-c24a0955d834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connect-->connect\n",
            "connected-->connect\n",
            "connecting-->connect\n",
            "connection-->connect\n",
            "compute-->comput\n",
            "computer-->comput\n",
            "computing-->comput\n",
            "computation-->comput\n",
            "run-->run\n",
            "running-->run\n",
            "runner-->runner\n",
            "runs-->run\n",
            "analysis-->analysi\n",
            "analyzing-->analyz\n",
            "analyzed-->analyz\n",
            "analytical-->analyt\n",
            "teach-->teach\n",
            "teacher-->teacher\n",
            "teaching-->teach\n",
            "taught-->taught\n",
            "play-->play\n",
            "playing-->play\n",
            "played-->play\n",
            "player-->player\n",
            "happy-->happi\n",
            "happiness-->happi\n",
            "happily-->happili\n",
            "study-->studi\n",
            "studies-->studi\n",
            "studying-->studi\n",
            "studied-->studi\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer  = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"-->\"+stemmer.stem(word))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHosO5FwYmQ3"
      },
      "source": [
        "# Stemming is being done. RegexpStemmer is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1VvoXvzYtoJ",
        "outputId": "6cafd377-943d-4851-ca26-4588501d3161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connect->connect\n",
            "connected->connect\n",
            "connecting->connect\n",
            "connection->connection\n",
            "compute->comput\n",
            "computer->computer\n",
            "computing->comput\n",
            "computation->computation\n",
            "run->run\n",
            "running->runn\n",
            "runner->runner\n",
            "runs->run\n",
            "analysis->analysi\n",
            "analyzing->analyz\n",
            "analyzed->analyz\n",
            "analytical->analytical\n",
            "teach->teach\n",
            "teacher->teacher\n",
            "teaching->teach\n",
            "taught->taught\n",
            "play->play\n",
            "playing->play\n",
            "played->play\n",
            "player->player\n",
            "happy->happy\n",
            "happiness->happines\n",
            "happily->happily\n",
            "study->study\n",
            "studies->studie\n",
            "studying->study\n",
            "studied->studi\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "stemmer = RegexpStemmer('ing$|s$|e$|able$|ed$', min=4)\n",
        "for word in words:\n",
        "  print(word+\"->\"+stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQFj3eKhaEUq"
      },
      "source": [
        "# SnowBallStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cELxmp59Y1zB",
        "outputId": "6288c884-fb0a-4569-ffdc-3ddf8b50567d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connect->connect\n",
            "connected->connect\n",
            "connecting->connect\n",
            "connection->connect\n",
            "compute->comput\n",
            "computer->comput\n",
            "computing->comput\n",
            "computation->comput\n",
            "run->run\n",
            "running->run\n",
            "runner->runner\n",
            "runs->run\n",
            "analysis->analysi\n",
            "analyzing->analyz\n",
            "analyzed->analyz\n",
            "analytical->analyt\n",
            "teach->teach\n",
            "teacher->teacher\n",
            "teaching->teach\n",
            "taught->taught\n",
            "play->play\n",
            "playing->play\n",
            "played->play\n",
            "player->player\n",
            "happy->happi\n",
            "happiness->happi\n",
            "happily->happili\n",
            "study->studi\n",
            "studies->studi\n",
            "studying->studi\n",
            "studied->studi\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "for word in words:\n",
        "  print(word+\"->\"+stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
