{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRw4Gp4nbdrZWKbZdkE5xy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohith7612/Generative-AI/blob/main/Tokeization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSka0SPuSaT_",
        "outputId": "b67d5b3e-941a-43c1-cf0c-5c789664aacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "Hello! My name is Rohith. I’m learning NLP, Machine Learning, and Compiler Design.\n",
        "Tokenisation helps computers understand text.\n",
        "Let’s test: How many tokens are in this sentence?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ifubYE5PSfJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization is being done. Corpus is being converted to sentences."
      ],
      "metadata": {
        "id": "0gqnKIIGTFuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents = sent_tokenize(corpus)\n",
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO6O7W69Slv7",
        "outputId": "d9c471d2-43ad-44c3-ca38-4d334e4fc16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello!\n",
            "My name is Rohith.\n",
            "I’m learning NLP, Machine Learning, and Compiler Design.\n",
            "Tokenisation helps computers understand text.\n",
            "Let’s test: How many tokens are in this sentence?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenization is being done. Documents is being converted to words."
      ],
      "metadata": {
        "id": "ol8jqh1zUCDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "id": "f4CjikqSSu_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e46a9b-5b44-4204-89c9-5e0362f3ebf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Rohith', '.']\n",
            "['I', '’', 'm', 'learning', 'NLP', ',', 'Machine', 'Learning', ',', 'and', 'Compiler', 'Design', '.']\n",
            "['Tokenisation', 'helps', 'computers', 'understand', 'text', '.']\n",
            "['Let', '’', 's', 'test', ':', 'How', 'many', 'tokens', 'are', 'in', 'this', 'sentence', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pdp5JeIVRWt",
        "outputId": "2f807f07-3b65-465b-ddf0-0b344bf6f189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Rohith', '.']\n",
            "['I', '’', 'm', 'learning', 'NLP', ',', 'Machine', 'Learning', ',', 'and', 'Compiler', 'Design', '.']\n",
            "['Tokenisation', 'helps', 'computers', 'understand', 'text', '.']\n",
            "['Let', '’', 's', 'test', ':', 'How', 'many', 'tokens', 'are', 'in', 'this', 'sentence', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Vb0ZZQVWWT",
        "outputId": "13fb46dd-3152-4e3c-de2f-69e25366a7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Rohith.',\n",
              " 'I’m',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " ',',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " ',',\n",
              " 'and',\n",
              " 'Compiler',\n",
              " 'Design.',\n",
              " 'Tokenisation',\n",
              " 'helps',\n",
              " 'computers',\n",
              " 'understand',\n",
              " 'text.',\n",
              " 'Let’s',\n",
              " 'test',\n",
              " ':',\n",
              " 'How',\n",
              " 'many',\n",
              " 'tokens',\n",
              " 'are',\n",
              " 'in',\n",
              " 'this',\n",
              " 'sentence',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming is being done. PorterStemmer is being used."
      ],
      "metadata": {
        "id": "gLKFxO4EYLAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\n",
        "    \"connect\", \"connected\", \"connecting\", \"connection\",\n",
        "    \"compute\", \"computer\", \"computing\", \"computation\",\n",
        "    \"run\", \"running\", \"runner\", \"runs\",\n",
        "    \"analysis\", \"analyzing\", \"analyzed\", \"analytical\",\n",
        "    \"teach\", \"teacher\", \"teaching\", \"taught\",\n",
        "    \"play\", \"playing\", \"played\", \"player\",\n",
        "    \"happy\", \"happiness\", \"happily\",\n",
        "    \"study\", \"studies\", \"studying\", \"studied\"\n",
        "]"
      ],
      "metadata": {
        "id": "qpSiDlyzVb6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer  = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"-->\"+stemmer.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXYhwMDYXysQ",
        "outputId": "d79a87c8-9b8e-4645-d1bd-c24a0955d834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connect-->connect\n",
            "connected-->connect\n",
            "connecting-->connect\n",
            "connection-->connect\n",
            "compute-->comput\n",
            "computer-->comput\n",
            "computing-->comput\n",
            "computation-->comput\n",
            "run-->run\n",
            "running-->run\n",
            "runner-->runner\n",
            "runs-->run\n",
            "analysis-->analysi\n",
            "analyzing-->analyz\n",
            "analyzed-->analyz\n",
            "analytical-->analyt\n",
            "teach-->teach\n",
            "teacher-->teacher\n",
            "teaching-->teach\n",
            "taught-->taught\n",
            "play-->play\n",
            "playing-->play\n",
            "played-->play\n",
            "player-->player\n",
            "happy-->happi\n",
            "happiness-->happi\n",
            "happily-->happili\n",
            "study-->studi\n",
            "studies-->studi\n",
            "studying-->studi\n",
            "studied-->studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming is being done. RegexpStemmer is being used."
      ],
      "metadata": {
        "id": "sHosO5FwYmQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "stemmer = RegexpStemmer('ing$|s$|e$|able$|ed$', min=4)\n",
        "for word in words:\n",
        "  print(word+\"->\"+stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1VvoXvzYtoJ",
        "outputId": "6cafd377-943d-4851-ca26-4588501d3161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connect->connect\n",
            "connected->connect\n",
            "connecting->connect\n",
            "connection->connection\n",
            "compute->comput\n",
            "computer->computer\n",
            "computing->comput\n",
            "computation->computation\n",
            "run->run\n",
            "running->runn\n",
            "runner->runner\n",
            "runs->run\n",
            "analysis->analysi\n",
            "analyzing->analyz\n",
            "analyzed->analyz\n",
            "analytical->analytical\n",
            "teach->teach\n",
            "teacher->teacher\n",
            "teaching->teach\n",
            "taught->taught\n",
            "play->play\n",
            "playing->play\n",
            "played->play\n",
            "player->player\n",
            "happy->happy\n",
            "happiness->happines\n",
            "happily->happily\n",
            "study->study\n",
            "studies->studie\n",
            "studying->study\n",
            "studied->studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SnowBallStemmer"
      ],
      "metadata": {
        "id": "fQFj3eKhaEUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "for word in words:\n",
        "  print(word+\"->\"+stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cELxmp59Y1zB",
        "outputId": "6288c884-fb0a-4569-ffdc-3ddf8b50567d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connect->connect\n",
            "connected->connect\n",
            "connecting->connect\n",
            "connection->connect\n",
            "compute->comput\n",
            "computer->comput\n",
            "computing->comput\n",
            "computation->comput\n",
            "run->run\n",
            "running->run\n",
            "runner->runner\n",
            "runs->run\n",
            "analysis->analysi\n",
            "analyzing->analyz\n",
            "analyzed->analyz\n",
            "analytical->analyt\n",
            "teach->teach\n",
            "teacher->teacher\n",
            "teaching->teach\n",
            "taught->taught\n",
            "play->play\n",
            "playing->play\n",
            "played->play\n",
            "player->player\n",
            "happy->happi\n",
            "happiness->happi\n",
            "happily->happili\n",
            "study->studi\n",
            "studies->studi\n",
            "studying->studi\n",
            "studied->studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization. Uses parts of speech to convert into root form."
      ],
      "metadata": {
        "id": "x4Re6uEeeykW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in words:\n",
        "  print(word+\"->\"+lemmatizer.lemmatize(word,pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2n4vudCe2qR",
        "outputId": "cb272b46-66e6-4700-e4c1-f3306107487c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connect->connect\n",
            "connected->connect\n",
            "connecting->connect\n",
            "connection->connection\n",
            "compute->compute\n",
            "computer->computer\n",
            "computing->compute\n",
            "computation->computation\n",
            "run->run\n",
            "running->run\n",
            "runner->runner\n",
            "runs->run\n",
            "analysis->analysis\n",
            "analyzing->analyze\n",
            "analyzed->analyze\n",
            "analytical->analytical\n",
            "teach->teach\n",
            "teacher->teacher\n",
            "teaching->teach\n",
            "taught->teach\n",
            "play->play\n",
            "playing->play\n",
            "played->play\n",
            "player->player\n",
            "happy->happy\n",
            "happiness->happiness\n",
            "happily->happily\n",
            "study->study\n",
            "studies->study\n",
            "studying->study\n",
            "studied->study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words and its process"
      ],
      "metadata": {
        "id": "G7qT2siwgysw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech = \"\"\"My dear young friends, I stand before you today with great hope and confidence in the power of youth.\n",
        "Each one of you has a unique role to play in shaping the destiny of our nation, and this responsibility\n",
        "demands hard work, integrity, and continuous learning. Dreams are not mere thoughts that appear during\n",
        "sleep, but powerful visions that inspire you to think beyond limitations and act with courage.\n",
        "When you set a clear goal and work with discipline and determination, challenges will transform into\n",
        "opportunities for growth. Education is not just about acquiring knowledge, but about igniting curiosity,\n",
        "building character, and developing the ability to serve society. If the youth of India commit themselves\n",
        "to innovation, ethical leadership, and national development, our country will surely emerge as a\n",
        "knowledge-driven and self-reliant nation.\"\"\""
      ],
      "metadata": {
        "id": "IxXONua8fGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords\n",
        "documents = sent_tokenize(speech)\n",
        "for i in range(len(documents)):\n",
        "  words = word_tokenize(documents[i])\n",
        "  words = [lemmatizer.lemmatize(word.lower(),pos = \"v\") for word in words if word not in stopwords.words('english')]\n",
        "  documents[i] = ' '.join(words)\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cJovs0Ng7AC",
        "outputId": "d9b3d576-521e-47c1-fcde-80b3780c5a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my dear young friends , i stand today great hope confidence power youth .', 'each one unique role play shape destiny nation , responsibility demand hard work , integrity , continuous learn .', 'dream mere thoughts appear sleep , powerful visions inspire think beyond limitations act courage .', 'when set clear goal work discipline determination , challenge transform opportunities growth .', 'education acquire knowledge , ignite curiosity , build character , develop ability serve society .', 'if youth india commit innovation , ethical leadership , national development , country surely emerge knowledge-driven self-reliant nation .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abguIwG4hV0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}